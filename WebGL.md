# WebGL相关知识点
#### WebGL2是如何工作的
WebGL 和 GPU 到底在做什么
CPU 基本做了两部分事情： 
第一部分是处理顶点(数据流)，变成裁剪空间节点；
第二部分是基于第一部分的结果绘制像素

点着色器是用GLSL写的函数 每个顶点都用调用一次它。

对于每个像素，都会调用片段着色器。 它有一个 vec4 类型的输出变量，它指示绘制像素的颜色是什么

缓冲区是将顶点和将每个顶点数据传给GPU的方法
 gl.createBuffer创建一个缓冲区。 
 gl.bindBuffer将该缓冲区设置为正在处理的缓冲区。 
 gl.bufferData将数据复制到当前缓冲区中。

#### z buffer
Z缓冲器算法也叫深度缓冲器算法，属于图像空间消隐算法
该算法有帧缓冲器和深度缓冲器
intensity（x，y）——属性数组（帧缓冲器） 存储图像空间每个可见像素的光强或颜色
depth（x，y）——深度数组（z-buffer）存放图像空间每个可见像素的z坐标
算法思想：先将Z缓冲器中各单元的初始值置为最小值。
当要改变某个像素的颜色值时，首先检查当前多边形的深度值是否大于该像素原来的深度值（保存在该像素所对应的Z 缓冲器的单元中）
如果大于原来的z值，说明当前多边形更靠近观察点，用它的颜色替换像素原来的颜色
z-Buffer算法的优点：
Z-Buffer算法比较简单，也很直观
在象素级上以近物取代远物。与物体在屏幕上的出现 顺序是无关紧要的，有利于硬件实现
z-Buffer算法的缺点：
占用空间大
没有利用图形的相关性与连续性，这是z-buffer算法 的严重缺陷
更为严重的是，该算法是在像素级上的消隐算法

只用一个深度缓存变量zb的改进算法，存深度最小值，需要判断象素点(i,j)是否在pk的投影多边形之内

#### 点与多边形的包含性检测：
射线法
由被测点P处向 y = -∞方向作射线
交点个数是奇数，则被测点在 多边形内部
交点个数是偶数表示在多边形外部

弧长法
以p点为圆心，作单位圆，把边投影到单位圆上，对应一段段 弧长，规定逆时针为正，顺时针为负，计算弧长代数和
代数和为0，点在多边形外部
代数和为2π，点在多边形内部
代数和为π，点在多边形边上

以顶点符号为基础的弧长累加方法
#### 物体拾取的原理
射线追踪法
基于颜色的像素判断法


#### 深度测试
深度值存储在每个片段里面（作为片段的z值），
当片段想要输出它的颜色时，将它的深度值和z缓冲进行比较，
如果当前的片段在其它片段之后，它将会被丢弃，否则将会覆盖。
这个过程称为深度测试(Depth Testing)

#### 帧缓冲
#### 几何: 怎么判断点在三角形内
#### 实例化渲染
#### 模型的优化: draco
#### 性能优化
#### 不同相机模型
#### 大量物体的渲染方法:合并
#### mv矩阵
#### 灯光

#### 3d的graphic pipeline
Application CPU负责，把各种几何体信息输送到Geometry阶段，如点、线、三角形。
    这个阶段是不需要细分子管线的，因为是在CPU处理
    假设是单线程，那么并没有管线处理的能力
    不过可以用多核多线程，来实现并行计算
Geometry
    model and view transform
    vertex shading
    projection
    clipping
    screen mapping
Rasterizer 
    triangle setup
    triangle traversal 
        fragment是指被光栅化阶段的三角形覆盖(全覆盖或部分覆盖）到的像素格子其中之一
        找出每个三角形的fragments，就叫triangle traversal，或叫scan conversion
    pixel shading
    merging
        depth test和stencil test发生在这个阶段
#### MVP变换详解（物体从自身坐标系到屏幕渲染之间发生的矩阵转换）
MVP变换：将我们已经构建好的各种3维模型映射到屏幕这个2维坐标中
参与 MVP 变换的信息包括点、矢量、法线、切线等
模型变换（Model）：将模型空间转换到世界空间
观察变换（View）：将世界空间转换到观察空间
投影变换（Projection）：将观察空间转换到裁剪空间
最后要获取屏幕坐标还需要一步：屏幕映射，又叫视口变换
屏幕映射：获取对应屏幕的 2D 坐标

模型变换(Model)
本质就是旋转，平移，缩放
模型自身会携带模型坐标的原点信息等，再结合世界坐标还是很好变换的

观察变换(View)
在我的理解下，M 和 V 变换本质上是同一种类型的变换
比较快速的方法是把整个摄像机坐标空间移动到世界坐标使二者重合再反转 z 轴（不是真的移动摄像机），即在物体与摄像机相对位置不变的情况下让摄像机位于（0，0，0），将变换摄像机的矩阵作用于物体上就可得到观察空间中的坐标

投影(Projection)
投影要为裁剪做准备
视野是有限的，所以在视野外（视锥外）的东西是不需要显示的
而用六个裁剪平面（视锥的六个面）直接判断相对复杂
所以需要投影操作将视锥变成裁剪空间，再通过齐次除法，
将裁剪空间变成CVV(Canonical View Volume)，
OpenGL中CVV就是一个[-1,1]^3的正方体（下面讨论这种），
DirectX中的 z 分量稍微不同是[0,1]，
此时的坐标就是 NDC(Normalized Device Coordinates，归一化设备坐标)
不考虑 z 轴的情况下，投影是将所有点投影到近裁剪平面上（个人认为可以理解为摄像机拍到的画面，一张特殊的二维相片），然后放缩到[-1,1]^2的正方形平面
